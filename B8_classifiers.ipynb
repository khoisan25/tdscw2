{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import math\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "from operator import neg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "\n",
    "import scipy\n",
    "\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = \"ttds_stopwords.txt\"\n",
    "sentiment_train_path = \"train-sentiment.txt\"\n",
    "sentiment_test_path = \"ttds_2023_cw2_test_final.txt\"\n",
    "\n",
    "with open(stopwords, 'r') as f:\n",
    "    stopwords = set(map(str.strip, f.readlines()))\n",
    "    \n",
    "    \n",
    "def join_to_string(data_list):\n",
    "    # Flatten the list of lists into a single list\n",
    "    flattened_list = [\"\\t\".join(map(str, item)) for item in data_list]\n",
    "    \n",
    "    # Join all items in the list into a single string\n",
    "    joined_string = '\\n'.join(map(str, flattened_list))\n",
    "    return joined_string\n",
    "\n",
    "    \n",
    "sentiment_train = pd.read_csv(sentiment_train_path, sep='\\t').values.tolist()\n",
    "sentiment_test = pd.read_csv(sentiment_test_path, sep='\\t').values.tolist()\n",
    "sentiment_test_final = pd.read_csv(\"ttds_2023_cw2_test_final.txt\", sep='\\t').values.tolist()\n",
    "\n",
    "random.shuffle(sentiment_train)\n",
    "random.shuffle(sentiment_test)\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "dev_ratio = 0.1\n",
    "\n",
    "train_section_end =  int(len(sentiment_train) * train_ratio)\n",
    "test_section_end = int(train_section_end + (len(sentiment_train) * test_ratio))\n",
    "dev_section_start = test_section_end\n",
    "\n",
    "train_set = sentiment_train[:train_section_end]\n",
    "test_set = sentiment_train[train_section_end:test_section_end]\n",
    "dev_set = sentiment_train[test_section_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, embedding_dim=20, window_size=2, learning_rate=0.01, epochs=10):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.oov_embedding = np.random.rand(1, embedding_dim)\n",
    "\n",
    "    def _build_vocab(self, tokenized_tweets):\n",
    "        word_counts = defaultdict(int)\n",
    "        for tweet in tokenized_tweets:\n",
    "            for word in tweet:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        self.word_to_index = {word: i for i, word in enumerate(word_counts)}\n",
    "        self.index_to_word = {i: word for word, i in self.word_to_index.items()}\n",
    "\n",
    "    def _generate_training_data(self, tokenized_tweets):\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for tweet in tokenized_tweets:\n",
    "            tweet_indices = [self.word_to_index[word] for word in tweet if word in self.word_to_index]\n",
    "            for i, word_index in enumerate(tweet_indices):\n",
    "                for j in range(max(i - self.window_size, 0), min(i + self.window_size + 1, len(tweet_indices))):\n",
    "                    if i != j:\n",
    "                        X.append(word_index)\n",
    "                        Y.append(tweet_indices[j])\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    def train(self, tokenized_tweets):\n",
    "        #self._build_vocab(tokenized_tweets)\n",
    "        #X, Y = self._generate_training_data(tokenized_tweets)\n",
    "\n",
    "        #vocab_size = len(self.word_to_index)\n",
    "        #self.W1 = np.random.rand(vocab_size, self.embedding_dim)\n",
    "        #self.W2 = np.random.rand(self.embedding_dim, vocab_size)\n",
    "        if not self.word_to_index:\n",
    "            self._build_vocab(tokenized_tweets)\n",
    "        X, Y = self._generate_training_data(tokenized_tweets)\n",
    "\n",
    "        vocab_size = len(self.word_to_index)\n",
    "        self.W1 = np.random.rand(vocab_size, self.embedding_dim)\n",
    "        self.W2 = np.random.rand(self.embedding_dim, vocab_size)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = np.zeros(vocab_size)\n",
    "                x[X[i]] = 1\n",
    "                h = np.dot(self.W1.T, x)\n",
    "                u = np.dot(self.W2.T, h)\n",
    "                y_pred = np.exp(u) / sum(np.exp(u))\n",
    "\n",
    "                e = -np.zeros_like(y_pred)\n",
    "                e[Y[i]] = 1\n",
    "                e -= y_pred\n",
    "\n",
    "                dW2 = np.outer(h, e)\n",
    "                dW1 = np.outer(x, np.dot(self.W2, e))\n",
    "\n",
    "                self.W1 += self.learning_rate * dW1\n",
    "                self.W2 += self.learning_rate * dW2\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch} completed\")\n",
    "        \n",
    "    def get_word_embedding(self, word):\n",
    "        if word in self.word_to_index:\n",
    "            return self.W1[self.word_to_index[word]]\n",
    "        else:\n",
    "            return self.oov_embedding\n",
    "        \n",
    "    def get_tweet_embedding(self, tweet_tokens):\n",
    "        embeddings = [self.get_word_embedding(word) for word in tweet_tokens]\n",
    "        if embeddings:\n",
    "            tweet_embedding = np.mean(np.array(embeddings), axis=0)\n",
    "        else:\n",
    "            # Return a zero vector if there are no words in the tweet\n",
    "            tweet_embedding = self.oov_embedding\n",
    "        return tweet_embedding\n",
    "\n",
    "    #def get_tweet_embedding(self, tweet_tokens):\n",
    "    #    embeddings = [self.get_word_embedding(word) for word in tweet_tokens]\n",
    "    #    tweet_embedding = np.mean(embeddings, axis=0)\n",
    "    #    #np.isnan(embedding).any() and not np.isinf(embedding).any()\n",
    "    #    if np.isnan(tweet_embedding).any():\n",
    "    #        return self.oov_embedding\n",
    "    #    return tweet_embedding\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = SkipGram()\n",
    "tokenised_sample = [list(tokeniser.tokenise(tweet[2])) for tweet in sample]\n",
    "# reemove stopwords\n",
    "tokenised_sample = [list(filter(lambda x: x not in stopwords, tweet)) for tweet in tokenised_sample]\n",
    "skipgram_model.train(tokenised_sample)\n",
    "# Get embedding for a specific word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = skipgram_model.get_word_embedding(\"and\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model.get_word_embedding(\"buyers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size=64, output_size=3):\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = {0: np.zeros((self.Whh.shape[0], 1))}\n",
    "        h = self.last_hs[0]\n",
    "\n",
    "        for i, x in enumerate(inputs):\n",
    "            x = x.reshape(-1, 1)\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        return y.flatten()\n",
    "\n",
    "    def backward(self, d_y, learn_rate=0.01):\n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        # Calculate dWhy and dby\n",
    "        dWhy = np.dot(d_y, self.last_hs[n].T)\n",
    "        dby = d_y\n",
    "\n",
    "        # Initialize dWhh, dWxh, and dbh to zero\n",
    "        dWhh = np.zeros(self.Whh.shape)\n",
    "        dWxh = np.zeros(self.Wxh.shape)\n",
    "        dbh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Calculate d_h for the last h\n",
    "        d_h = np.dot(self.Why.T, d_y)\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(n)):\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "            dbh += temp\n",
    "            dWhh += np.dot(temp, self.last_hs[t].T)\n",
    "            # Correct the input reshape\n",
    "            input_t = self.last_inputs[t].reshape(self.Wxh.shape[1], 1)\n",
    "            dWxh += np.dot(temp, input_t.T)\n",
    "\n",
    "            d_h = np.dot(self.Whh.T, temp)\n",
    "\n",
    "        # Clip to prevent exploding gradients\n",
    "        for d in [dWxh, dWhh, dbh, dWhy, dby]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        self.Wxh -= learn_rate * dWxh\n",
    "        self.Whh -= learn_rate * dWhh\n",
    "        self.bh -= learn_rate * dbh\n",
    "        self.Why -= learn_rate * dWhy\n",
    "        self.by -= learn_rate * dby\n",
    "\n",
    "\n",
    "# Custom loss function: Categorical Cross-Entropy\n",
    "def categorical_cross_entropy(predictions, targets):\n",
    "    predictions = np.clip(predictions, 1e-12, 1. - 1e-12)\n",
    "    ce = -np.sum(targets * np.log(predictions + 1e-9)) / targets.shape[0]\n",
    "    return ce\n",
    "\n",
    "# Convert categorical labels to one-hot\n",
    "def to_one_hot(labels, dimension=3):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1\n",
    "    return results\n",
    "\n",
    "# Function to convert words to vectors using Word2Vec\n",
    "def word_to_vec(word, model):\n",
    "    return model.wv[word] if word in model.wv else np.zeros(model.vector_size)\n",
    "\n",
    "# Initialize RNN\n",
    "rnn = SimpleRNN(input_size=dims, hidden_size=8, output_size=3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(25):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    for i, tweet in enumerate(tokenized_corpus_train):\n",
    "        inputs = np.array([word_to_vec(word, model) for word in tweet])\n",
    "        target = to_one_hot([ytrain[i]], dimension=3)\n",
    "\n",
    "        # Forward pass\n",
    "        out = rnn.forward(inputs)\n",
    "        probs = np.exp(out) / np.sum(np.exp(out), axis=0, keepdims=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = categorical_cross_entropy(probs, target[0])\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        d_y = probs - target[0]\n",
    "        rnn.backward(d_y.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(tokenized_corpus_train)}\")\n",
    "\n",
    "\n",
    "# Preparing the data for logistic regression\n",
    "def prepare_data_rnn(corpus, rnn_model):\n",
    "    return [rnn_model.forward(np.array([word_to_vec(word, model) for word in tweet])) for tweet in corpus]\n",
    "\n",
    "xtrain_rnn = prepare_data_rnn(tokenized_corpus_train, rnn)\n",
    "xdev_rnn = prepare_data_rnn(tokenized_corpus_dev, rnn)\n",
    "xtest_rnn = prepare_data_rnn(tokenized_corpus_test, rnn)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(solver='liblinear', class_weight='balanced', penalty='l1', max_iter=1000)\n",
    "logistic_model.fit(xtrain_rnn, ytrain)\n",
    "\n",
    "# Making predictions\n",
    "ytrain_pred = logistic_model.predict(xtrain_rnn)\n",
    "ydev_pred = logistic_model.predict(xdev_rnn)\n",
    "ytest_pred = logistic_model.predict(xtest_rnn)\n",
    "\n",
    "# Printing accuracies\n",
    "print(\"Training accuracy:\", accuracy_score(ytrain, ytrain_pred))\n",
    "print(\"Development accuracy:\", accuracy_score(ydev, ydev_pred))\n",
    "print(\"Testing accuracy:\", accuracy_score(ytest, ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming 'dims' is the size of your Word2Vec vectors\n",
    "# max_len = ... # Define max_len based on your dataset\n",
    "\n",
    "# CNN Model\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=5, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=5, padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change shape to [batch, channels, sequence_length]\n",
    "        x = self.bn1(self.relu(self.conv1(x)))\n",
    "        #x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = torch.max(x, 2)[0]  # Global Max Pooling\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x #self.softmax(x)\n",
    "\n",
    "dims = 128\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TextCNN(input_dim=dims, num_classes=3)  # Assuming 3 classes for your problem\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "        \n",
    "        train_accuracy = evaluate_model(model, train_loader)\n",
    "        print(f'Training Set Accuracy: {train_accuracy}')\n",
    "        \n",
    "        dev_accuracy = evaluate_model(model, dev_loader)\n",
    "        print(f'Development Set Accuracy: {dev_accuracy}')\n",
    "        \n",
    "        test_accuracy = evaluate_model(model, test_loader)\n",
    "        print(f'Test Set Accuracy: {test_accuracy}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computations\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            true_labels.extend(labels.numpy())\n",
    "            predictions.extend(predicted.numpy())\n",
    "\n",
    "    return accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Prepare your dev and test sets in a similar way as the training set\n",
    "# Assuming xdev, ydev, xtest, ytest are already prepared\n",
    "\n",
    "# Create TensorDatasets and DataLoaders for dev and test sets and train \n",
    "train_data = TensorDataset(xtrain, torch.tensor(ytrain))\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "dev_data = TensorDataset(xdev, torch.tensor(ydev))\n",
    "dev_loader = DataLoader(dev_data, batch_size=128, shuffle=False)\n",
    "\n",
    "test_data = TensorDataset(xtest, torch.tensor(ytest))\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "dev_accuracy = evaluate_model(model, dev_loader)\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "train_accuracy = evaluate_model(model, train_loader)\n",
    "\n",
    "print(f'Training Set Accuracy: {train_accuracy}')\n",
    "print(f'Development Set Accuracy: {dev_accuracy}')\n",
    "print(f'Test Set Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.bias = bias\n",
    "\n",
    "        self.attention = nn.Linear(feature_dim, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        eij = self.attention(x)\n",
    "        \n",
    "        eij = torch.tanh(eij)\n",
    "        eij = eij.squeeze(2)\n",
    "        alpha = torch.softmax(eij, dim=1)\n",
    "        \n",
    "        # Weight input with attention weights\n",
    "        weighted_input = x * alpha.unsqueeze(2)\n",
    "        return torch.sum(weighted_input, 1), alpha\n",
    "\n",
    "class TextCNNWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TextCNNWithAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.attention = Attention(128, 35)  # Assuming 35 is the max sequence length\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_x, _ = self.attention(x.permute(0, 2, 1))\n",
    "        \n",
    "        x = self.fc1(attn_x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "dims = 128\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TextCNNWithAttention(input_dim=dims, num_classes=3)  # Assuming 3 classes for your problem\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "        \n",
    "        train_accuracy = evaluate_model(model, train_loader)\n",
    "        print(f'Training Set Accuracy: {train_accuracy}')\n",
    "        \n",
    "        dev_accuracy = evaluate_model(model, dev_loader)\n",
    "        print(f'Development Set Accuracy: {dev_accuracy}')\n",
    "        \n",
    "        test_accuracy = evaluate_model(model, test_loader)\n",
    "        print(f'Test Set Accuracy: {test_accuracy}')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
